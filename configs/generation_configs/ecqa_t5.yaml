task:
  type: preprocessing-generation
  data_dir: data/ecqa/removal_format=${RATIONALE_FORMAT}_ng=${NUM_NGRAMS}_mf=${MIN_FREQ}_mt=${MAX_TOKENS}/
  output_dir: data/ecqa/generation_format=${RATIONALE_FORMAT}_ng=${NUM_NGRAMS}_mf=${MIN_FREQ}_mt=${MAX_TOKENS}_th=${THRESHOLD}/
  vocab_path: data/ecqa_vocabs/vocab_format=${RATIONALE_FORMAT}_ng=${NUM_NGRAMS}_mf=${MIN_FREQ}_mt=${MAX_TOKENS}.pt
  model_name: t5-base
  attribution_model:
    type: biencoding-lstm-from-best
    path: ckpt/ecqa_${REMOVAL_MODEL_TYPE}_format=${RATIONALE_FORMAT}_ng=${NUM_NGRAMS}_mf=${MIN_FREQ}_mt=${MAX_TOKENS}
  explainer:
    type: ig-lstm
    num_steps: 20
    max_input_length: 512
    max_output_length: 32
    device: "cuda:0"
  explainer_preprocessor:
    type: ecqa-global-explanation-preprocessor
    rationale_format: ${RATIONALE_FORMAT}
    batch_size: 1
  generation_collate_fn:
    type: ecqa-infilling-collate-fn
    rationale_format: ${RATIONALE_FORMAT}
    max_input_length: 256
    max_output_length: 32
    removal_threshold: ${THRESHOLD}
    intervention_on_label: False